\section{Evaluating the new solver} \label{se:ver}
%Currently, there is no standard utility in POP for evaluating the effect of code modifications (such as a new solver) that will yield non bit-for-bit (BFB) results but should still produce the same mean climate.
Due to the chaotic nature of the ocean dynamics, even a round-off difference from the barotropic solver may potentially result in
distinct model solutions.
Therefore, because we cannot guarantee bit-for-bit (BFB) identical results in ocean solutions when a new solver is introduced,
we needed to show that the use of P-CSI with EVP did not result in inaccuracies (or even a changed climate)
before it could be formally incorporated into a POP release.

When POP is ported to a new machine, a similar situation occurs where running the same simulation on the two machines is not expected to produce BFB results.
The existing POP procedure to verify that a port to a new machine was successful involves running a specific case on the new machine for five simulation days, and
then computing the root-mean-square error (RMSE) between the new solution and the standard dataset released by NCAR for the SSH (sea surface height) field.
%There is no direct verification tool for new solvers  in CESM POP currently, but it provides a way to facilitate the evaluation of a successful port on new machines.
%That is, to run a specific case on the new machine for five days, then compute the root-mean-square (RMS) difference of SSH field between the solution on local machine and those released as standard dataset by NCAR.
While this procedure provides a simple criterion for evaluating CESM results on new machines (which may contain errors due to the software or hardware environment),
we found that it was insufficient for detecting and evaluating solver-induced errors.
For example, we ran the 1\degree\space case for three years with different convergence tolerances varying from $10^{-10}$ to $10^{-16}$ in the barotropic solver (default is $10^{-13}$)
and calculated the RMSE between a given case and the most strict tolerance case ($10^{-16}$).
Figure \ref{fig:ssh_rmse_t} shows the RMSE for the temperature field with various convergence tolerances for each month, and clearly
error introduced by modifying the solver convergence tolerance is not revealed in the temperature field (nor was it evident in any of the other diagnostic fields, such as velocity and SSH).
We had expected that the simulations with tolerances of $10^{-10}$ and $10^{-11}$ would have larger RMSE values than the others.
However, this was not the case, and, during months twelve and twenty, the $10^{-10}$ case has almost the smallest RMSE.
%In the last two months, the $10^{-11}$ case has a smaller RMSE than all other cases except the $10^{-10}$ case.
Note that to isolate the effect of the linear solver, we only looked at error in the open seas (POP does not simulate well on several marginal seas).


%While RMSE can reveal an averaged difference between two cases, it is not sufficient to determine whether that difference is indicative of an altered climate.
Because the existing simple RMSE test was insufficient for detecting whether the climate had been altered, we developed an alternative to evaluating the new ocean solver using a statistical approach.  Rather than relying on a single simulation, an ensemble of simulations can better represent the natural variability of the chaotic climate simulation, as described in \cite{baker2014methodology} in the context of data compression for the CESM Community Atmosphere Model (CAM), and be
used as a baseline for evaluating non-BFB modifications.  Similar to \cite{baker2014methodology}, we create an ensemble of simulations which are identical to the default setup except for an order $10^{-14}$ perturbation in the initial ocean temperature. This perturbation size is not expected to produce different climate model states.  We found that an ensemble of size 40 was sufficient for our purposes to represent the variability in the ocean, and we ran longer simulations than for CAM (12-months) due to the longer time-scales present in the ocean. Also note that we ultimately chose to evaluate only the three-dimensional temperature field (instead of the two-dimensional SSH) as we found it to be the most useful diagnostic variable for revealing differences.





%However, it is not a good criterion for new algorithms because it does not take into consideration of the chaotic nature of climate models.
%In order to make the chaotic nature accounted, we employ the methodology as proposed in CESM atmosphere component CAM by Baker et al. \cite{baker2014methodology}. First we conduct an ensemble of runs which are identical to the original case except for a $10^{-14}$ perturbation in the initial temperature. This perturbation is a reasonable round-off error which climate model should be able to tolerate.

We determine whether the new result is consistent with the reference ensemble results as follows.
We define the ensemble output at time $T$ as $\mathcal{E}=\{X_1,X_2,...,X_m\}$, where
$m$ is the size of the ensemble.
At a given point $j$, we have a series of possible results for each variable $X$ from the ensemble $\{X_1(j),X_2(j),...,X_m(j)\}$.
As the ensemble size increases, this series more correctly reflects the distribution of reasonable realization at the given point.
We define the mean and standard deviation of this series at point $j$ as $\mu (j) $ and
$\delta (j)$, respectively.
 %as  $$ \mu (j) = \frac{1}{m}\sum_{i=1}^m X_i(j), $$
%and standard deviation as  $$ \delta (j) = \sqrt{\frac{1}{m} \sum_{i=1}^m (X_i(j)-\mu(j))^2 }.$$
Let the new case have the result $\tilde{X}$, 
then the root-mean-square Z-score indicates the average error between the new case and the ensemble data:

$$ RMSZ(\tilde{X}, \mathcal{E}) =  \sqrt{\frac{1}{n}\sum_{j=1}^n(\frac{\tilde{X}(j) -\mu (j)}{\delta (j)})^2}$$
%The combination of $\mu$ and $\delta$ provides a criterion to test whether an additional case is close to the ensemble or not.
%Set the additional case has the result $\tilde{x}$, define the root-mean-square Z-score
We then re-evaluated the various solver tolerances using the ensemble-based RMSZ measurement.
Figure \ref{fig:ssh_rmsz_t} indicates that, unlike the simple RMSE test, the new ensemble-based method
is able to identify larger errors due to less strict convergence tolerances.  Now
the two cases with the loosest tolerances clearly have RMSZ scores on the same order as the error
they introduced into the solver and are noticeably removed from the ensemble distribution.
This success led us to use the ensemble-based metric to evaluate our new solver and find that the P-CSI results were consist with those of the ensemble (as were the default and stricter tolerances).

\begin{figure}[!t]
\begin{center}
\includegraphics[height=6.5cm]{temp_rmse.eps}
\end{center}
\vspace{-.1in}
\caption[] {Monthly Root Mean Square Error (RMSE) of temperature for cases with different convergence tolerances in 1\degree\space POP.}
\label{fig:ssh_rmse_t}
\vspace{-.1in}
\end{figure}
\begin{figure}[!t]
\begin{center}
\includegraphics[height=6.5cm]{temp_rmsz.eps}
\end{center}
\vspace{-.1in}
\caption[] {Monthly Root Mean Square Z-score for temperature for cases with different convergence tolerances. The yellow area represent the range of RMSZ within the 40-member ensemble.}
\vspace{-.1in}
\label{fig:ssh_rmsz_t}
\end{figure}


 %cases provides a criterion to judge whether the case  is consistent with the them or not.
%Also, for those cases which depart so far away from the ensemble, such as the cases with the first and second largest tolerance, their RMSZ is in the same order as the order of error they introduced to the solver.
%So the error introduced by replacing ChronGear with PCG is in the same magnitude order of improving the convergence tolerance from $10^{-13}$ to $10^{-14}$ or $10^{-16}$.
