\section{Experimental results} \label{se:exp}
%----------------------------------------------------------------------------
%\subsection{Experimental Platform} \label{se:plat}

We first evaluate the performance of our new barotropic solver in
CESM1.2.0 on the Yellowstone supercomputer, located at NCAR-Wyoming
Supercomputing Center (NWSC) \cite{loft:2015}. Yellowstone uses 
2.6-Ghz Intel Xeon E5-2670 ``Sandy Bridge'' processors providing a total of  72,576 cores that are connected
by a 13.6 GBps InfiniBand network.  Obtaining good performance on
Yellowstone is critical as its role is to support atmospheric sciences
and more than 50\% of its usage is due to CESM \cite{wf2014}
%Processors are 2.6-GHz Intel Xeon E5-2670 with Advanced Vector Extensions (AVX). 
To focus on the performance of POP, we use the CESM
``G\_NORMAL\_YEAR'' component set which uses active ocean and sea
ice components (the atmosphere component is data-driven). 
%ocean and ice with COREv2 normal year forcing is used.
We examine  the two most frequently-used POP horizontal grid resolutions: 
1\degree\space ($320\times 384$) and 0.1\degree\space ($3600\times 2400$).
Note that by default, CESM1.2.0 sets Yellowstone's MPI environment
eager limit (MP\_EAGER\_LIMIT), which controls the maximum message size before a rendezvous protocol is used,
to zero.  We discovered that by using the default eager limit on
Yellowstone (MP\_EAGER\_LIMIT = 131072) instead, POP performance significantly improves.


\subsection{Low-resolution simulations}
The execution times for the barotropic mode with available solvers in 1\degree\space POP on
Yellowstone are shown in Figure \ref{fig:runtime1}.  With the default
diagonal preconditioning, P-CSI out-performs ChronGear on all core
counts and reduces the solver execution time from 0.58s to 0.41s per
simulation day (1.4x speedup) at the highest core count (768).
Further, with the new
block EVP preconditioner, convergence is improved for both the ChronGear
and P-CSI solvers at higher core counts.  At 768 cores, P-CSI with EVP achieves 0.37s per simulation
day, which is an 1.6x improvement over the original ChronGear solver
with diagonal preconditioning.
\begin {figure}[!t]
\centering
\includegraphics[height =6.5cm]{NEW1deg_solverruntime}
\vspace{-.25in}
\caption []{Execution times for the barotropic mode in 1\degree\space POP
  for one simulation day.\label {fig:runtime1}}
\vspace{-.1in}
\end {figure}

\begin{table}[!h]
\begin{center}
\caption{Percent improvement of the total execution time for
  1\degree\space POP on Yellowstone. \label{tab:improve_1}}
\begin{scriptsize}
\begin{tabular}{|l||l|l|l|l|l|}
\hline
Number of cores & 48  & 96  & 192 & 384 & 768\\\hline
\hline
ChronGear+EVP & -.5\% & 1.1\%  & 6.5\% & 10.8\%  & 12.1 \% \\\hline
P-CSI+Diagonal  & .7\% &3.9\% &9.3\%  &11.0\% & 12.6 \% \\\hline
P-CSI+EVP	      &-2.4\% & .4\%	& 7.4\%  & 14.4\% & 16.7\%\\\hline
\end{tabular}
\end{scriptsize}
\vspace{-.2in}
\end{center}
\end{table}


The improvement of the barotropic solver reduces the total execution time for the entire
POP model.  Table \ref{tab:improve_1} lists the percentage
improvement of POP for the three new solver/preconditioner options
compared to POP with the diagonal-preconditioned ChronGear solver. 
Times were obtained from a 5-day simulation, with model initialization
and I/O excluded. P-CSI with a block-EVP preconditioner yields a 16.7\% improvement on 768 processor cores.  
While a 16.7\% improvement may seem modest, POP at 1\degree\space resolution is commonly run for multi-century timescales.  Such an improvement may translate into the saving of millions of CPU hours.  Further,
the 1\degree\space resolution needs to be run at (relatively) high core counts
when POP is configured with biogeochemistry mode due to the many additional
tracers required.

\subsection{High-resolution simulations}
\begin {figure*}[t!]
\begin{center}
\includegraphics[height =6.5cm]{NEW01deg_solverruntime}
\hspace{10pt}
\includegraphics[height =6.5cm]{NEW01deg_speedup_ys}
\end{center}
\vspace{-.2in}
\caption []{Execution times for the barotropic mode in 0.1\degree\space POP
  for one simulation day on Yellowstone (left). 
The core simulation rates of 0.1\degree\space POP on Yellowstone (right).\label {fig:runtime01}}
%\vspace{-.2in}
\end {figure*}


Now we test the scalability of the new barotropic solver in
high-resolution 0.1\degree\space POP on Yellowstone.  At this resolution,
the choice of ocean block size and layout, which affects the
distribution of work across processors, has a large impact on
performance.  Therefore, to remove this influence from our scaling
results, we were careful to specify block decompositions for each core
count with the same aspect ratio (3:2) and land ratio (.25) and to use
space-filling curves.  We use the default timestep for
0.1\degree\space POP, which is 500 time steps per day (dt\_count = 500).
Finally, for the sake of consistency, for all solvers we checked for convergence every 10 iterations.
Note that because P-CSI iterations are relatively inexpensive
(compared to performing the POP  convergence check), 
P-CSI performance may improve if the check for convergence occurs less frequently.


As shown in Figure \ref{fig:runtime01} (left),
ChronGear performance begins to degrade after about 2700 cores,
while the execution time for P-CSI becomes relatively flat at that
point. With diagonal preconditioning, P-CSI accelerates the barotropic
mode in 0.1\degree\space POP by 4.3x (from 19.0s to 4.4s per simulation
day) on 16,875 cores.  EVP preconditioning further improves the
performance of both ChronGear and P-CSI, resulting in a speedup of the
original barotropic mode by 1.4x and 5.2x, respectively.
In Section \ref{se:baro}, we demonstrated that the original barotropic
solver takes an increasing percentage of POP execution time as the
number of cores increases. In particular, on 16,875 cores, ChronGear
with diagonal preconditioning accounts for about 50\% of the total
execution time.  In contrast, Figure \ref{fig:StepComp_pcsi}
illustrates the improvement of the barotropic mode with the more-scalable EVP preconditioned
P-CSI solver, which constitutes only about 16\% of the total execution
time on 16,875 cores. 


Improvement of the barotropic solver benefits the
overall performance of POP, especially at large core counts. 
Simulation rate (simulated years per wall-clock day) is a popular
criterion for climate model performance, and here we use the core
simulation rate (i.e., the execution time excluding initialization and I/O costs).
%Figure \ref{fig:runtime01} (right) shows that diagonal preconditioned and EVP
%preconditioned P-CSI solver accelerate the entire POP simulation by
%1.66x and 1.74x, respectively.
A simulation rate of 5 simulated years per wall-clock day is considered
the minimum rate required to run long term climate simulations
\cite{dennis2012computational}, and Figure \ref{fig:runtime01} (right)
shows that P-CSI can attain higher rates than ChronGear.
The EVP-preconditioned P-CSI solver improves the core simulation rate of
POP by 1.7x on 16,875 cores, from 6.2 to 10.5 simulated years per wall-clock day.

\begin {figure}[t!]
\centering
%\vspace{-10pt}
%\includegraphics[width=1.0\linewidth]{POPStepComp_pcsi.eps}
\includegraphics[height=6.5cm]{NEWPOPStepComp_pcsi.eps}
\caption[] {Percentage of execution time in 0.1\degree\space POP using
  P-CSI with block-EVP preconditioning.\label{fig:StepComp_pcsi}}
\end{figure}




To illustrate the source of improvement, more detailed timing
information for the barotropic solvers is provided in Figure
\ref{fig:component}.  Figure \ref{fig:component} indicates that P-CSI
outperforms ChronGear primarily due to fewer global reductions. The
reduction in global reductions will also significantly reduce the
sensitivity of POP to operating system noise \cite{ferreira} by increasing the time between global synchronization.  In
addition, the block-EVP preconditioner reduces the boundary update
costs by reducing the number of iterations required.  Computation
costs for the barotropic solver are negligible compared to the global
reduction and boundary update costs at large core
counts, and, therefore, the extra computations (almost double)
required by the EVP preconditioner have little to no impact.  Finally, note that the global reduction time actually decreases at less than 1200 cores which is consistent with the theoretical results in equation \ref{t_pcg} and \ref{t_psi}.

%Finally,
%note
%that the global reduction time decreases at the smallest core counts
%(less than 1200), which conflicts with the theoretical result in
%equation \ref{t_pcg} and \ref{t_psi}.
%This behavior can be attributed to a load imbalance. 
%When fewer numbers of cores are used in POP, 
%the specified block size is larger on each process. 
%When the blocks containing only land are eliminated, 
%the bigger block sizes lead to greater load imbalances between cores.

%Note that for smaller core counts (e,g., less than 4,000) global
%reduction and boundary communication times do not decrease with the
%EVP preconditioner (instead of diagonal) for P-CSI.  
%At small core counts, the EVP
%preconditioner exacerbates this load imbalance because of the near
%doubling of computation time.  Thus, even though the iteration number
%is reduced to a half in the case of EVP preconditioned P-CSI, each
%communication takes longer to complete.


\subsection{Simulations on Edison}
Now we run the 0.1\degree\space POP simulations on the Edison supercomputer
to verify that performance improvements are not unique to Yellowstone.
Edison, which is the newest supercomputer at the National Energy
Research Scientific Computing Center (NERSC), consists of 133,824 2.4
GHz Intel ``Ivy Bridge'' processor cores connected by an 8GBps Cray
Aries high-speed interconnect with Dragonfly topology.
% and has a peak performance of 2.57 PFLOPS.
\begin {figure*}[t!]
\begin{center}
\includegraphics[height =7cm]{01deg_comp_all_gs}
\hspace{10pt}
\includegraphics[height =7cm]{01deg_comp_all_halo}
\end{center}
\vspace{-.2in}
\caption[] {Execution times for the major components of the
  barotropic solvers in 0.1\degree\space POP on Yellowstone:  global
  reduction (left) and boundary communication (right). }
\label{fig:component}
%\vspace{-.2in}
\end {figure*}
\begin {figure*}[t!]
\begin{center}
\includegraphics[height=6.5cm]{01deg_solverruntime_edison}
\hspace{10pt}
\includegraphics[height=6.5cm]{01deg_speedup_edison}
\end{center}
\vspace{-.2in}
\caption []{Execution times for the barotropic mode in  0.1\degree\space POP
  for one simulation day on Edison (left). 
The core simulation rate of 0.1\degree\space POP on Edison (right).\label {fig:runtime01_edison}}
\vspace{-.2in}
\end {figure*}

Figure \ref{fig:runtime01_edison} shows that simulations on Edison
with the four solver configurations have similar performance
characteristics as on
Yellowstone. 
%ChronGear stops scaling after 2,700 cores, while P-CSI scales well until 4,220 cores are used. 
We note that we encountered much more variability in the global
communication times in our simulations on Edison (as compared to
Yellowstone), likely due to network contention \cite{wang2014}. As a result, the
ChronGear times (with both preconditioners) varied a lot from run to
run, so we took the average of the best three results to represent the
execution time.  Because P-CSI has hardly any global reductions
(only in the convergence check),
the variability in those runs was small. 
%(and on par with the Yellowstone runs).
%It is worth mentioning that global communication is more unstable on
%Edison than on Yellowstone. 
%As a result, the execution time of ChronGear with either a diagonal
%preconditioner or an EVP preconditioner varies a lot from run to run. 
%Here we use the average of the best three results to represent the execution time. 
On Edison,  P-CSI with diagonal preconditioning in  0.1\degree\space POP
accelerates the barotropic mode by 3.7x (from 26.2s to 7.0s per simulation day) on 16,875 cores.
With EVP preconditioning, both ChronGear and P-CSI performance
improves, and the combination of P-CSI and EVP preconditioning results in a 5.6x  speedup. 
 
%\begin{table}
%%\vspace{-10pt}
%\begin{center}
%\begin{scriptsize}
%\caption {Core simulation rates obtained for 0.1\degree\space POP, with the
%  preconditioned ChronGear (CG) and P-CSI solvers. \label{tab:improve_01}}
%\begin{tabular}{|l||r|r|r|r|r|r|r|}
%%\toprule
%\hline
%Solver & 470  & 1200   & 2700 & 4220 & 7500 & 10800 & 16875\\\hline
%\hline
%%CG     &0.71 &1.68&3.38  &4.62 &6.02 &6.07 &5.34\\\hline
%%CG+Evp &0.70 &1.69&3.44  &4.88 &6.62 &6.89 &6.46\\\hline
%%P-CSI+Diag    &0.72 &1.72&3.51  &5.00 &7.04 &8.29 &8.85\\\hline
%%P-CSI+Evp     &0.70 &1.69&3.49  &5.01 &7.23 &8.55 &9.27\\
%CG     &0.7 &1.7&3.4  &4.6 &6.0 &6.0 &5.3\\\hline
%CG+Evp &0.7 &1.7&3.4  &4.9 &6.6 &6.9 &6.5\\\hline
%P-CSI+Diag    &0.7 &1.7&3.5  &5.0 &7.0 &8.3 &8.9\\\hline
%P-CSI+Evp     &0.7 &1.7&3.5  &5.0 &7.2 &8.6 &9.3\\
%\hline
%%\bottomrule
%\end{tabular}
%\end{scriptsize}
%\vspace{-.2in} 
%\end{center}
%\end{table}
