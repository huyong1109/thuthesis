%\section{Error Vector Propagation (EVP) Preconditioning} \label{se:evp}
\section{A Block-EVP preconditioner} \label{se:evp}

%Equations (\ref{t_pcg}) and (\ref{t_psi}) show that
The total
execution time of the barotropic solver is the product of the number
of iterations and the execution time per iteration.  With increasing
numbers of cores, the execution time of computation in each iteration
decreases, but the execution time of communication increases.  To
reduce communication costs, preconditioning is commonly used to reduce
the number of iterations to convergence, assuming the cost of
preconditioning is reasonable.
%Three major factors affecting a
%preconditioner's cost for large sparse linear systems are efficiency,
%scalability and preprocessing requirements.
While the current ChronGear
solver in POP has benefited greatly by using a simple diagonal
preconditioner \cite{pini1990simple, reddy2013comparison},
further improvement to its convergence rate
would significantly reduce the associated communication cost and
improve scalability.
In fact, the performance of both P-CSI and the existing ChronGear solvers could be
further improved with a more effective preconditioner.

%The data distribution and communication cost can only tolerate lower level of factorization like ILU(0). L and U have the same nonzero structure as the lower and upper parts of A respectively, which often results in a crude approximation has a very bad effect on convergence speed.
%More accurate ILU factorizations is not suit for ocean model, because it requires more communications and computations.
%Also, the preprocessing cost to compute the factors is higher.

%In 1985, Concus et al. \cite{concus1985block} used the banded approximate of the matrix inverse as a preconditioner,
%and achieved higher efficiency than other preconditioners on elliptic partial differential equations.
%NOTE YH what is "local approximate inverse of the true operator"?
%Smith et al. \cite{smith1992parallel} employed a polynomial preconditioning method and a local approximate-inverse method.
%POP supports a preconditioner consisting of a 9-point operator which is a local approximate-inverse of the true operator.

\subsection{Block preconditioning}

% The most widely used preconditioning techniques in sequential simulations are the incomplete factorization methods
% like incomplete LU factorization (ILU) and its variants \cite{benzi2002preconditioning}.
% However, they are ill-suited for parallel computers because they require a recursive operation which limits parallelization.

Some parallelizable preconditioning methods such as polynomial,
approximate-inverse, multigrid, and block preconditioning have
drawn much attention recently.  High-order polynomial preconditioning
can reduce iterations as effectively as incomplete LU factorization
(ILU) and its variants \cite{benzi2002preconditioning} in sequential
simulations.  However, the computational overhead for polynomial
preconditioners typically offsets its superiority to diagonal
preconditioning (e.g., \cite{meyer1989numerical,smith1992parallel}),
as a $k$th-order polynomial preconditioner requires $k$ matrix
vector multiplications in each iteration. Approximate-inverse
preconditioners, while highly parallelizable, require solving
a linear system several times larger than the original system 
\cite{smith1992parallel,bergamaschi2007numerical}, which makes it less
attractive for POP than a simple diagonal preconditioner.  
%Algebraic multigrid (AMG) is highly scalable and effective for linear systems
%derived from elliptic systems of equations (e.g.,
%\cite{baker2012scaling}), but a previous investigation into using an AMG
%preconditioner in POP found the setup costs to
%be too high to be competitive with diagonal preconditioning.
Multigrid is highly scalable and generally effective for linear
systems derived from elliptic systems of equations. Recent works
indicate that geometric multigrid (GMG) is promising in atmosphere
\cite{muller2014massively} and ocean
\cite{matsumura2008non,kanarska2007algorithm} modeling when uniform
grids and simple topography are involved. However,  in global ocean
models, the presence of 
complex topography (such as islands, straits, passages and coastal complexities) combined with non-uniform or anisotropic grids
%non-smooth topographic boundaries and non-uniform or anisotropic grids 
result in less than ideal scaling for simple GMG methods
\cite{matsumura2008non,fulton1986multigrid,tseng2003ghost,stuben2001review}.
Note that in CESM-POP, general dipole orthogonal grids are used to
avoid the polar singularity, and only the ocean part of the earth
surface is simulated with masked lands. These choices lead to an
elliptic system with variable coefficients defined on an irregular
domain with non-uniform grids, making the effective use of GMG
non-trivial.  The scenario is even worse for the high resolution ocean
grid where thousands of islands and narrow passages 
are not representable at a grid coarsening of even one or two levels
(e.g., the Bering Strait). For complex geometries, algebraic multigrid (AMG) is often a viable
alternative to GMG.  A drawback of AMG, though, is that in some cases,
the setup cost can exceed that of iterative solver itself, making it
inferior to a well-preconditioned CG method (e.g.,
\cite{muller2014massively}), particularly when the number of CG
iterations is reasonably low as for CESM-POP (Figure
\ref{fig:iteration} in the next section). 
%However, the setup of AMG is too costly
%that its cost sometimes exceeds that of iterative solver itself, which
%makes it inferior to properly preconditioned CG method in some cases
%\cite{muller2014massively}.  
Further, because POP requires solving the linear
equation tens and hundreds of times per simulation day in the low and
high resolution versions, respectively, and thousands of
simulation years are needed in the typical simulation, the
costly setup of AMG is prohibitive.
%is too costly to be competitive with diagonal preconditioning.
 % Multigrid is
% another popular preconditioning method which is highly scalable and
% efficient in preconditioning the sparse linear systems developed from
% elliptic equations \cite{baker2012scaling}.
% But the cost of setting
% up the multigrid preconditioning is so high that it is not worth using
% multigrid for iterative solvers in problems like POP, which converge
% in tens
%or hundreds of iterations.
Finally, block preconditioning has been shown to be an effective
parallel preconditioner (e.g., \cite{concus1985block, white2011block})
and is appealing for POP because it makes use of the block structure of the
coefficient matrix that arises from discretization of the
elliptic equations.
% factorizes the matrix block-by-block instead of point-by-point.

\begin {figure}
\centering
\includegraphics[height=5.0cm]{blockpreconditioning.eps}
\caption[] {Sparsity pattern of the coefficient matrix developed from nine-point stencils.
The whole domain is divided into $3\times3$ non-overlapping blocks.
Elements in red rectangles are coefficients between points in blocks.
Elements in blue rectangles are coefficients between points from the $i$-th block and its neighbor blocks. \label{fig:blockprecond}}
\vspace{-.2in}
\end{figure}

To facilitate describing the new block EVP preconditioner, we first
briefly review a general block preconditioner, as illustrated by Figure
\ref{fig:blockprecond}.  If the linear system of $\mathcal{N} \times
\mathcal{N}$ grid points is reordered block-by-block with size of $n\times n$
(e.g., $\mathcal{N}/3\times \mathcal{N}/3$ in
Figure \ref{fig:blockprecond}), then coefficient matrix $A$ can be
represented by a nine-diagonal block matrix.
Each row of this matrix contains nine sub-matrices.
% (Fig \ref{fig:blockprecond}). %(arranged in their spatial relationship)
Each $B_i$ (red blocks) is a block matrix containing coefficients of
the grid points in the i-th block, which share the same structure as $A$ but
have a smaller size ($n^2\times n^2$).  $B_i^e$, $B_i^w$, $B_i^n$ and
$B_i^s$ are block matrices containing coefficients of points
on east, west, north and south boundaries and the points on their
respective neighboring blocks, thus having at most $3n$ nonzero
elements distributed on $n$ rows. $B_i^{nw}$, $B_i^{ne}$, $B_i^{sw}$
and $B_i^{se}$ have only one nonzero element, representing the
coefficient of corner points and their neighboring points on the
northwest, northeast, southwest and southeast blocks.  The traditional
block preconditioning method constructs the approximate inverse of $A$
by sequentially factorizing it with approximations of $B_i^{-1}$,
which is ill-suited for parallel applications.  In contrast, the
inverse of the block diagonal of $A$, which provides a good
approximation for $A$, can be calculated naturally in parallel.  The
inverse of the diagonal block matrices is
\begin{eqnarray*}
M^{-1}=    \left [
        \begin{array}{ccccccc}
        B_1^{-1} &   &  \\
         & \ddots&  \\
        &   &  B_{m^2}^{-1} \\
    \end{array}
    \right ]
\end{eqnarray*}
Using $M$ as a preconditioner, the preconditioning process $\textbf{x}
= M^{-1}\textbf{y}$ is typically transformed into solving the sparse
linear equations $B_i \textbf{x}_i = \textbf{y}_i$ for each block,
(instead of explicitly constructing $B_i^{-1}$) and solving via LU decomposition.
The arithmetic complexity of solving these equations with LU decomposition is $\mathcal{O}(n^4)$,
providing that the LU decomposition is previously initialized.


% This parallel block
% preconditioning is preferable in parallel application due to less
% computation to solve the equations with LU decomposition than
% multiplying $\textbf{y}$ by $M^{-1}$.

\subsection{Error Vector Propagation method}
In contrast with LU decomposition, the arithmetic complexity of solving the equations $B_i \textbf{x}_i =\textbf{y}_i$ is $\mathcal{O}(n^2)$,
where $B_i$ is $n^2\times n^2$, when using the Error Vector
Propagation (EVP) method,
which to the best of our knowledge is one of the least costly algorithms
for solving elliptic equations in serial \cite{roache1995elliptic}.
%The EVP method marches over the whole domain based on the discretized equations with an arithmetic complexity $\mathcal{O}(n^2)$.
%EVP is an alternative of fast direct solvers for elliptic equations,
The EVP method and its variants have been used in several ocean models
(e.g., Sandia Ocean Modeling System \cite{dietrich1987ocean} and
CANadian version of DIEcast \cite{sheng1998candie}).  Further, Tseng and Chien
\cite{tseng2011parallel} employed a modified parallel EVP
method based on domain-decomposition as a solver for the global ocean
simulation.

\begin {figure}[!t]
\centering
%\includegraphics[width=0.8\linewidth]{evp9pmarch1.png}
\includegraphics[height=5.0cm]{evp9pmarch1.png}
\caption []{EVP marching method for nine-point stencil. The solution on point $(i+1,j+1)$ can be calculated using the equation on point $(i,j)$, providing solutions on other neighbor points of point $(i,j)$.  \label {fig:evp9p}}
\vspace{-.2in}
\end {figure}

The EVP method works as follows. We discretize Equation \ref{eq:ssh}
(the implicit elliptic system of equations for SSH)
into the following form so that we can march the solution northeastward assuming all other neighboring points are exactly known :
\begin{eqnarray}
\label{eq:evp9p}
&\eta_{i+1,j+1} = (1/A_{i,j}^{ne} )(\psi_{i,j} - A_{i,j}^0\eta_{i,j}-A_{i,j}^e\eta_{i+1,j} \nonumber\\
&-A_{i,j}^n\eta_{i,j+1}-A_{i-1,j}^{ne}\eta_{i-1,j+1} +A_{i-1,j}^e\eta_{i-1,j}\nonumber\\
&-A_{i-1,j-1}^{ne}\eta_{i-1,j-1}-A_{i,j-1}^n\eta_{i,j-1}- A_{i+1,j-1}^{ne}\eta_{i,j-1} )
\end{eqnarray}
%\textbf{I suggest you to make Fig. 5 to use a different color or label to represent the specified boundary conditions}
Figure \ref{fig:evp9p} illustrates a Dirichlet boundary
elliptic equation $\mathcal{B}\textbf{x} = \psi$ on a small domain.  We
define the interior points next to the south and west boundaries as
the initial guess points $\textbf{e}$ and those next to the north and
east boundaries are the final boundary points $\textbf{f}$ (e.g.,
$\textbf{e}= \{E_1, \dots, E_7\}$, $\textbf{f}= \{F_1, \dots, F_7\}$
in Figure \ref{fig:evp9p}).  If the true solution on $\textbf{e}$ is
known, the exact values over the whole domain can be computed
sequentially from southwest to northeast corners, using Equation
\ref{eq:evp9p}. This procedure is referred to as marching.
Unfortunately, the value on $\textbf{e}$ is often not known until the
elliptic equation is solved.  However, we can get a solution
$\textbf{x}$ satisfying the elliptic equation on the whole domain
except on the boundary, by first guessing the value
$\textbf{x}|_\textbf{e}$ on $\textbf{e}$ and then calculating the rest
using the marching method.  Then $E=(\textbf{x} -\eta)|_\textbf{e}$
and $F=(\textbf{x} -\eta)|_\textbf{f}$ are error vectors on
$\textbf{e}$ and $\textbf{f}$, respectively.  The error vector $F$ is
already known since $\textbf{f}$ are boundary points (Dirichlet
boundary condition is imposed).  The relationship between the error on
initial guess points and the final boundary points can be represented
as $F=W*E$.  This influence coefficient matrix $W$ can be formed by
marching on the whole domain with unit vectors on the initial guess
points and zero residual value in the whole domain.  We summarized
the EVP algorithm for an elliptic equation with zero boundary in
Algorithm \ref{alg:evp}.


The EVP method contains two steps: preprocessing and solving. In
the preprocessing step, the influence coefficient matrix and its
inverse are computed, involving a calculation of $\mathcal{C}_{pre}=
(2n-5)* 9n^2 + (2n-5)^3 = \mathcal {O} (26n^3)$.  Obtaining the solution in the solve step requires
$\mathcal{C}_{evp}= 2* 9n^2 + (2n-5)^2 = \mathcal{O} (22n^2)$.  This
estimate indicates that EVP has lower
computational cost for the solver step than other direct solvers such as
LU.
%What is the cost per iteration for PCG - didn't quite get the below.
%or iterative methods such as PCG for elliptic equations
Therefore, EVP can be practical in real applications from a cost standpoint because
preprocessing is only needed once at the beginning to obtain the
influence coefficient matrix and its inverse.
\begin{algorithm}[t!]
\caption{Nine-point Error Vector Propagation method}
\label{alg:evp}
\begin{scriptsize}
\begin{algorithmic}[1]
\REQUIRE Residual $\psi$ associated with a domain containing $n\times n$ grid points, $k = size(\textbf{e})=2n-5$; \\
//\qquad \textit{preprocessing }
\STATE  $\textbf{x} = \textbf{0}$
\FOR {i = 1, k}
\STATE $\textbf{x}|_\textbf{e}(i) = 1$
\STATE $\textbf{x} = marching(\textbf{x},\textbf{0})$
\STATE $W(i,:) = \textbf{x}|_\textbf{f}$
\STATE $\textbf{x}|_\textbf{e}(i) = 0$
\ENDFOR
\STATE $R = inverse(W)$ \\
//\qquad \textit{solving }
\STATE $\textbf{x}= marching(\textbf{x},\psi)$
\STATE $F = (\textbf{x} - \eta)|_\textbf{f}$
\STATE $\textbf{x}|_\textbf{e} =\textbf{x}|_\textbf{e} - R*F$
\STATE $\textbf{x} = marching(\textbf{x},\psi)$
\end{algorithmic}
\end{scriptsize}
\end{algorithm}

\subsection{EVP as a parallel preconditioner}
The EVP method described above is an efficient option for
solving elliptic equations.  However, a major drawback of
EVP is that it cannot solve on a large domain without further
modifications due to numerical instabilities when marching
\cite{roache1995elliptic}.  But on a small domain up to the size of
$12\times 12$, EVP solves with an acceptable round-off error of
$\mathcal{O}(10^{-8})$ when double-precision floating-point is used.  Its
effectiveness on small domains and low-computational cost make EVP an
ideal method for parallel block preconditioning.
%thus making it possible to use the block diagonal of $A$ as a preconditioner.
%\textbf{not sure what the above sentence means mean EVP combined with block diagonal?}
Here, we develop a block preconditioning technique based on the EVP
method in each block to further improve the performance of the
barotropic solver in POP.  Each preconditioner step solves the
elliptic equations $B_i \textbf{x} = \textbf{y} (i=1,...,m^2)$ in
parallel.


The fact that EVP is not well-suited for large domains is not an issue for
large-scale parallel computing, where larger number of processors
result in smaller domains.  Furthermore, for our system of equations,
the coefficients related to north, south, east and west neighbors on
every point are one magnitude order smaller than the others. We found
that removing these coefficients reduces the cost of
EVP preconditioning by about a half without any significant
impact on the convergence rate when used with both ChronGear and
P-CSI.  As a result, the execution time of EVP preconditioning can be
expressed as $\mathcal{T'}_{p} = 14n^2\theta=
14\frac{\mathcal{N}^2}{p}\theta$.  The actual cost of
$\mathcal{T'}_{p}$ depends on the size of the local block, which
decreases as more processor cores are used.
%In the current version of POP, using non-diagonal preconditioning methods requires an additional boundary communication after the preconditioning in each iteration.
%However, we find that the later boundary communication can be skipped by utilizing the fact that the halo size is 2.
Thus, the total execution time for one ChronGear and P-CSI solver step
with block-EVP preconditioning are
\begin{eqnarray}
\label{t_evppcg}
&\mathcal{T'}_{cg}=\mathcal{K'}_{cg} (\mathcal{T'}_c + \mathcal{T'}_b+\mathcal{T'}_g )\nonumber \\
&=\mathcal{K'}_{cg} [31 \frac{\mathcal{N}^2}{p}\theta + \frac{8\mathcal{N}}{\sqrt{p}}\beta +(4+\log p)\alpha],
\end{eqnarray}
and
\begin{eqnarray}
\label{t_evppsi}
\mathcal{T'}_{pcsi} = \mathcal{K'}_{pcsi}(\mathcal{T'}_c + \mathcal{T'}_b ) \nonumber \\
= \mathcal{K'}_{pcsi}[26\frac{\mathcal{N}^2}{p}\theta+ 4\alpha + \frac{8\mathcal{N}}{ \sqrt{p}}\beta],
\end{eqnarray}
respectively.

\begin {figure}[!t]
\centering
\includegraphics[height=6.5cm]{iteration.eps}
\vspace{-.1in}
\caption[] {Average number of iterations for different barotropic solvers. \label{fig:iteration}}
\vspace{-.1in}
\end{figure}

The implementation of EVP preconditioning in POP significantly reduces
the number of iterations required for convergence for both the ChronGear and P-CSI solvers.
In particular,  Figure \ref{fig:iteration} demonstrates that EVP
preconditioning reduces the iteration count by about two-thirds for
both the 1\degree\space and 0.1 \degree\space resolutions, which is comparable to the
approximate-inverse preconditioner proposed in
\cite{smith1992parallel} (which is not implemented in CESM POP).
Although  EVP preconditioning doubles the computation in each iteration, it halves both global and boundary communications
which dominate in the barotropic execution time at large core counts.
Another advantage of EVP preconditioning is the low
preprocessing cost.
%EVP preconditioning matrix is totally parallelized, while traditional decomposition based preconditioning requires serial processing.
In 0.1\degree\space case, the cost of setting up the preconditioning matrix
is less than that of one call to the solver when 512 processor cores
are used, and this cost is further decreased when more processors are used.
Finally, we note that the 0.1\degree\space case requires fewer iterations than the
1\degree\space case, because the higher resolution POP grid has a 
ratio of longitude to latitude grid spacing that is closer to 1, resulting
in a smaller condition number for the coefficient matrix.
