

\subsection{EVP as a parallel preconditioner}
The EVP method described above is an efficient option for
solving elliptic equations.  However, a major drawback of
EVP is that it cannot solve on a large domain without further
modifications due to numerical instabilities when marching
\cite{roache1995elliptic}.  But on a small domain up to the size of
$12\times 12$, EVP solves with an acceptable round-off error of
$\mathcal{O}(10^{-8})$ when double-precision floating-point is used.  Its
effectiveness on small domains and low-computational cost make EVP an
ideal method for parallel block preconditioning.
%thus making it possible to use the block diagonal of $A$ as a preconditioner.
%\textbf{not sure what the above sentence means mean EVP combined with block diagonal?}
Here, we develop a block preconditioning technique based on the EVP
method in each block to further improve the performance of the
barotropic solver in POP.  Each preconditioner step solves the
elliptic equations $B_i \textbf{x} = \textbf{y} (i=1,...,m^2)$ in
parallel.


The fact that EVP is not well-suited for large domains is not an issue for
large-scale parallel computing, where larger number of processors
result in smaller domains.  Furthermore, for our system of equations,
the coefficients related to north, south, east and west neighbors on
every point are one magnitude order smaller than the others. We found
that removing these coefficients reduces the cost of
EVP preconditioning by about a half without any significant
impact on the convergence rate when used with both ChronGear and
P-CSI.  As a result, the execution time of EVP preconditioning can be
expressed as $\mathcal{T'}_{p} = 14n^2\theta=
14\frac{\mathcal{N}^2}{p}\theta$.  The actual cost of
$\mathcal{T'}_{p}$ depends on the size of the local block, which
decreases as more processor cores are used.
%In the current version of POP, using non-diagonal preconditioning methods requires an additional boundary communication after the preconditioning in each iteration.
%However, we find that the later boundary communication can be skipped by utilizing the fact that the halo size is 2.
Thus, the total execution time for one ChronGear and P-CSI solver step
with block-EVP preconditioning are
\begin{eqnarray}
\label{t_evppcg}
&\mathcal{T'}_{cg}=\mathcal{K'}_{cg} (\mathcal{T'}_c + \mathcal{T'}_b+\mathcal{T'}_g )\nonumber \\
&=\mathcal{K'}_{cg} [31 \frac{\mathcal{N}^2}{p}\theta + \frac{8\mathcal{N}}{\sqrt{p}}\beta +(4+\log p)\alpha],
\end{eqnarray}
and
\begin{eqnarray}
\label{t_evppsi}
\mathcal{T'}_{pcsi} = \mathcal{K'}_{pcsi}(\mathcal{T'}_c + \mathcal{T'}_b ) \nonumber \\
= \mathcal{K'}_{pcsi}[26\frac{\mathcal{N}^2}{p}\theta+ 4\alpha + \frac{8\mathcal{N}}{ \sqrt{p}}\beta],
\end{eqnarray}
respectively.

\begin {figure}[!t]
\centering
\includegraphics[height=6.5cm]{iteration.eps}
\vspace{-.1in}
\caption[] {Average number of iterations for different barotropic solvers. \label{fig:iteration}}
\vspace{-.1in}
\end{figure}

The implementation of EVP preconditioning in POP significantly reduces
the number of iterations required for convergence for both the ChronGear and P-CSI solvers.
In particular,  Figure \ref{fig:iteration} demonstrates that EVP
preconditioning reduces the iteration count by about two-thirds for
both the 1\degree\space and 0.1 \degree\space resolutions, which is comparable to the
approximate-inverse preconditioner proposed in
\cite{smith1992parallel} (which is not implemented in CESM POP).
Although  EVP preconditioning doubles the computation in each iteration, it halves both global and boundary communications
which dominate in the barotropic execution time at large core counts.
Another advantage of EVP preconditioning is the low
preprocessing cost.
%EVP preconditioning matrix is totally parallelized, while traditional decomposition based preconditioning requires serial processing.
In 0.1\degree\space case, the cost of setting up the preconditioning matrix
is less than that of one call to the solver when 512 processor cores
are used, and this cost is further decreased when more processors are used.
Finally, we note that the 0.1\degree\space case requires fewer iterations than the
1\degree\space case, because the higher resolution POP grid has a 
ratio of longitude to latitude grid spacing that is closer to 1, resulting
in a smaller condition number for the coefficient matrix.
