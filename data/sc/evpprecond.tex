%\section{Error Vector Propagation (EVP) Preconditioning} \label{se:evp}

%\textbf{I suggest you to make Fig. 5 to use a different color or label to represent the specified boundary conditions}
Figure \ref{fig:evp9p} illustrates a Dirichlet boundary
elliptic equation $\mathcal{B}\textbf{x} = \psi$ on a small domain.  We
define the interior points next to the south and west boundaries as
the initial guess points $\textbf{e}$ and those next to the north and
east boundaries are the final boundary points $\textbf{f}$ (e.g.,
$\textbf{e}= \{E_1, \dots, E_7\}$, $\textbf{f}= \{F_1, \dots, F_7\}$
in Figure \ref{fig:evp9p}).  If the true solution on $\textbf{e}$ is
known, the exact values over the whole domain can be computed
sequentially from southwest to northeast corners, using Equation
\ref{eq:evp9p}. This procedure is referred to as marching.
Unfortunately, the value on $\textbf{e}$ is often not known until the
elliptic equation is solved.  However, we can get a solution
$\textbf{x}$ satisfying the elliptic equation on the whole domain
except on the boundary, by first guessing the value
$\textbf{x}|_\textbf{e}$ on $\textbf{e}$ and then calculating the rest
using the marching method.  Then $E=(\textbf{x} -\eta)|_\textbf{e}$
and $F=(\textbf{x} -\eta)|_\textbf{f}$ are error vectors on
$\textbf{e}$ and $\textbf{f}$, respectively.  The error vector $F$ is
already known since $\textbf{f}$ are boundary points (Dirichlet
boundary condition is imposed).  The relationship between the error on
initial guess points and the final boundary points can be represented
as $F=W*E$.  This influence coefficient matrix $W$ can be formed by
marching on the whole domain with unit vectors on the initial guess
points and zero residual value in the whole domain.  We summarized
the EVP algorithm for an elliptic equation with zero boundary in
Algorithm \ref{alg:evp}.


The EVP method contains two steps: preprocessing and solving. In
the preprocessing step, the influence coefficient matrix and its
inverse are computed, involving a calculation of $\mathcal{C}_{pre}=
(2n-5)* 9n^2 + (2n-5)^3 = \mathcal {O} (26n^3)$.  Obtaining the solution in the solve step requires
$\mathcal{C}_{evp}= 2* 9n^2 + (2n-5)^2 = \mathcal{O} (22n^2)$.  This
estimate indicates that EVP has lower
computational cost for the solver step than other direct solvers such as
LU.
%What is the cost per iteration for PCG - didn't quite get the below.
%or iterative methods such as PCG for elliptic equations
Therefore, EVP can be practical in real applications from a cost standpoint because
preprocessing is only needed once at the beginning to obtain the
influence coefficient matrix and its inverse.
\begin{algorithm}[t!]
\caption{Nine-point Error Vector Propagation method}
\label{alg:evp}
\begin{scriptsize}
\begin{algorithmic}[1]
\REQUIRE Residual $\psi$ associated with a domain containing $n\times n$ grid points, $k = size(\textbf{e})=2n-5$; \\
//\qquad \textit{preprocessing }
\STATE  $\textbf{x} = \textbf{0}$
\FOR {i = 1, k}
\STATE $\textbf{x}|_\textbf{e}(i) = 1$
\STATE $\textbf{x} = marching(\textbf{x},\textbf{0})$
\STATE $W(i,:) = \textbf{x}|_\textbf{f}$
\STATE $\textbf{x}|_\textbf{e}(i) = 0$
\ENDFOR
\STATE $R = inverse(W)$ \\
//\qquad \textit{solving }
\STATE $\textbf{x}= marching(\textbf{x},\psi)$
\STATE $F = (\textbf{x} - \eta)|_\textbf{f}$
\STATE $\textbf{x}|_\textbf{e} =\textbf{x}|_\textbf{e} - R*F$
\STATE $\textbf{x} = marching(\textbf{x},\psi)$
\end{algorithmic}
\end{scriptsize}
\end{algorithm}

\subsection{EVP as a parallel preconditioner}
The EVP method described above is an efficient option for
solving elliptic equations.  However, a major drawback of
EVP is that it cannot solve on a large domain without further
modifications due to numerical instabilities when marching
\cite{roache1995elliptic}.  But on a small domain up to the size of
$12\times 12$, EVP solves with an acceptable round-off error of
$\mathcal{O}(10^{-8})$ when double-precision floating-point is used.  Its
effectiveness on small domains and low-computational cost make EVP an
ideal method for parallel block preconditioning.
%thus making it possible to use the block diagonal of $A$ as a preconditioner.
%\textbf{not sure what the above sentence means mean EVP combined with block diagonal?}
Here, we develop a block preconditioning technique based on the EVP
method in each block to further improve the performance of the
barotropic solver in POP.  Each preconditioner step solves the
elliptic equations $B_i \textbf{x} = \textbf{y} (i=1,...,m^2)$ in
parallel.


The fact that EVP is not well-suited for large domains is not an issue for
large-scale parallel computing, where larger number of processors
result in smaller domains.  Furthermore, for our system of equations,
the coefficients related to north, south, east and west neighbors on
every point are one magnitude order smaller than the others. We found
that removing these coefficients reduces the cost of
EVP preconditioning by about a half without any significant
impact on the convergence rate when used with both ChronGear and
P-CSI.  As a result, the execution time of EVP preconditioning can be
expressed as $\mathcal{T'}_{p} = 14n^2\theta=
14\frac{\mathcal{N}^2}{p}\theta$.  The actual cost of
$\mathcal{T'}_{p}$ depends on the size of the local block, which
decreases as more processor cores are used.
%In the current version of POP, using non-diagonal preconditioning methods requires an additional boundary communication after the preconditioning in each iteration.
%However, we find that the later boundary communication can be skipped by utilizing the fact that the halo size is 2.
Thus, the total execution time for one ChronGear and P-CSI solver step
with block-EVP preconditioning are
\begin{eqnarray}
\label{t_evppcg}
&\mathcal{T'}_{cg}=\mathcal{K'}_{cg} (\mathcal{T'}_c + \mathcal{T'}_b+\mathcal{T'}_g )\nonumber \\
&=\mathcal{K'}_{cg} [31 \frac{\mathcal{N}^2}{p}\theta + \frac{8\mathcal{N}}{\sqrt{p}}\beta +(4+\log p)\alpha],
\end{eqnarray}
and
\begin{eqnarray}
\label{t_evppsi}
\mathcal{T'}_{pcsi} = \mathcal{K'}_{pcsi}(\mathcal{T'}_c + \mathcal{T'}_b ) \nonumber \\
= \mathcal{K'}_{pcsi}[26\frac{\mathcal{N}^2}{p}\theta+ 4\alpha + \frac{8\mathcal{N}}{ \sqrt{p}}\beta],
\end{eqnarray}
respectively.

\begin {figure}[!t]
\centering
\includegraphics[height=6.5cm]{iteration.eps}
\vspace{-.1in}
\caption[] {Average number of iterations for different barotropic solvers. \label{fig:iteration}}
\vspace{-.1in}
\end{figure}

The implementation of EVP preconditioning in POP significantly reduces
the number of iterations required for convergence for both the ChronGear and P-CSI solvers.
In particular,  Figure \ref{fig:iteration} demonstrates that EVP
preconditioning reduces the iteration count by about two-thirds for
both the 1\degree\space and 0.1 \degree\space resolutions, which is comparable to the
approximate-inverse preconditioner proposed in
\cite{smith1992parallel} (which is not implemented in CESM POP).
Although  EVP preconditioning doubles the computation in each iteration, it halves both global and boundary communications
which dominate in the barotropic execution time at large core counts.
Another advantage of EVP preconditioning is the low
preprocessing cost.
%EVP preconditioning matrix is totally parallelized, while traditional decomposition based preconditioning requires serial processing.
In 0.1\degree\space case, the cost of setting up the preconditioning matrix
is less than that of one call to the solver when 512 processor cores
are used, and this cost is further decreased when more processors are used.
Finally, we note that the 0.1\degree\space case requires fewer iterations than the
1\degree\space case, because the higher resolution POP grid has a 
ratio of longitude to latitude grid spacing that is closer to 1, resulting
in a smaller condition number for the coefficient matrix.
