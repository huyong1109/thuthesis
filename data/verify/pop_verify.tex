 

 


\introduction \label{sec:intro}


 




 
\section{Background discussion} \label{sec:back}

\subsection{Current ocean model quality assurance testing}

   



\subsection{集合模拟一致性检测}









\subsection{Motivation}

While the solver verification work in \cite{yong2015} was illuminating和adequate for the task at hand, it prompted several questions that motivated our work to develop a more comprehensive technique such as CAM-ECT for evaluating CESM-POP data.  First, a key consideration for a more general POP verification tool was the concern over spatial variability in the ocean, which is much more pronounced than in the atmosphere. Both CAM-ECT as well as the RMSZ strategy in \cite{yong2015} evaluate the differences in terms of spatial averages,和it was unclear whether this approach would be sufficient to detect any accuracy issue or model error in the ocean.  Second, because CESM-POP has fewer independent diagnostic variables than the CAM, we re-visit the question of appropriate metrics for consistency evaluation.  Third, the selection of different metrics prompted further examination of the appropriate ensemble size.  While an ensemble of size 40 ensemble was sufficient for detecting linear solver errors in \cite{yong2015}, this dimension was not thoroughly explored in a more general context, particularly in light of the recommended ensemble size of 151 for CAM-ECT.  Finally, the difference in temporal scales between the ocean和atmosphere prompted us to investigate the required length of ensemble runs.  Because the time-scales in the ocean are slower than in the atmosphere, intuitively a longer ensemble length is needed for statistical consistency testing with CESM-POP.  Note, though, that the initial results in Fig. \ref{fig:rmsz_temp_ens} suggest that this presumption may not be true.  The RMSZs continue to decrease with time after a few months when the tolerances are small enough (i.e., tolerance is smaller than $10^{-12}$), suggesting the required length of simulation merits further investigation.  In fact, Fig. \ref{fig:rmsz_temp_ens} shows that this particular 1-degree ocean data is relatively deterministic和the initial differences begin to damp out (grow) after the first year when the tolerances are small (unrealistically large).


%----------------------------------------------------------------------------
%----------------------------------------------------------------------------
%----------------------------------------------------------------------------


\section{A new statistical consistency test for POP}\label{sec:newtest}

Expanding the CESM-ECT suite to include a consistency test for CESM-POP, which we denote POP-ECT, data requires determining an appropriate ensemble to represent ocean model variability和developing a methodology to address ocean-specific characteristics. We first discuss the ensemble creation. We then describe the new testing procedure.

\subsection{POP ensemble construction}\label{sec:ensemble}

%As with CAM data in \cite{baker2015}, 
We evaluate whether differences in CESM-POP output data are statistically significant (i.e., indicative of a changed climate state) by comparing to an ensemble of simulations representing an accepted ocean state.  Therefore, the first stage in applying the CESM-ECT approach to CESM-POP data is creating an appropriate ensemble. Clearly, the ensemble composition is critical to an effective test,和simulations should be produced on an accepted machine with an accepted version of CESM.  Note that as compared to CAM, the ocean model has fewer independent diagnostic variables: temperature (TEMP), sea surface height (SSH), salinity (SALT),和the zonal和meridional velocities with respect to the model grid (UVEL和VVEL, respectively).  Of these five variables, SSH is 2D和the remainder are 3D.

For POP-ECT, we create an ensemble $E$ of $N_{ens}$ simulations, denoted by $E =\{E_1, E_2, \dots, E_{N_{ens}} \}$, that differ only by an $\mathcal{O}(10^{-14})$ perturbation to the initial ocean temperature field.  This initial perturbation size is on the order of double-precision round-off error和should not be climate-changing.
% over the first several years.  
The size of the ensemble must be sufficient to create a representative distribution, but as small as possible to avoid high computational cost.  While we address in detail our choice of ensemble size later in Sect. \ref{sec:ens}, we use $N_{ens} \;=\; 40$ for the discussion in this section和the experiments in the next.  Our experiments indicate that this choice of ensemble size adequately represents the natural variability in the ocean for testing purposes.  The ensemble simulation data consists of monthly temporal averages at each grid point $i$ for the 5 POP diagnostic variables: TEMP, SSH, SALT, UVEL,和VVEL.  Each of these variable datasets $X$ contains $N_X$ grid points和is denoted by ${X} = \{ x_1, x_2, \dots, x_{N_X}\}$, 这里 $x_i$ is a scalar monthly average at grid point $i$.  Data is collected for $T$ months (i.e., $T$ time slices of monthly data).

%Volume averaging is used to weight each point - cells on the bottom are larger, but also have smaller RMSE because less is happening (slower time scales).  So by giving them larger weights, we have more equality in contribution to calculating  failures (check this)

%\subsection{Requirement for spatial considerations}\label{sec:spatial}

The next step in CESM-ECT approach is to characterize the ensemble distribution in a qualitative way to facilitate the evaluation of new runs.  This statistical description of the ensemble is stored in a so-called ensemble summary file和is associated with the CESM softare tag used to generate the ensemble simulations.  The history files from the $N_{ens}$ simulations do not need to be retained once the summary file has been created.  Recall that for CAM data, CAM-ECT calculates the global weighted-area mean for each variable from the annual temporal averages available at each grid point.  This calculation results in a distribution of $N_{ens}$ global means for each variable. However, this approach of using global weighted-area means will not be appropriate for ocean model data, as ocean variability is less uniform across the grid than atmospheric data.

For example, consider an ensemble of $N_{ens} \;=\; 40$ CESM simulations with a 1\degree\space resolution CESM-POP grid run for $T\;=\;36$ months.  In Fig. \ref{fig:SST_STD_all}, we show spatial plots of the standard deviations of the sea surface temperature (SST), across the ensemble members after 1, 12, 24,和36 months.  Note that the SST is simply the top layer of the 3D variable TEMP (more precisely, the top 10 meters of upper ocean).  Figure \ref{fig:SST_STD_all} shows that the standard deviation is far from uniform with orders of magnitude differences across the grid (see the color bar scale).  Also notable is the change from 1 month to 12 months associated with the model instability 
associated with the development of hydrodynamic instability of the flow as the model spins up.
At the end of one year, the larger standard deviation in the tropical regions suggest a larger uncertainty due to the growth of tropical instability waves \citep{legeckis1977} in the ensembles.  The large variability can be easily enhanced in the equatorial regions because of the finer resolution in the tropics (approximately $1/3^\circ$).  There are also some pools of large standard deviation in the downstream of the major ocean current systems.  The change from 12 months to 36 is more subtle, indicating the associated physical instability may not grow further due to the ocean dissipation.  Given the range in variability evident in Fig. \ref{fig:SST_STD_all}, the RMSZ score strategy as used for verification in \cite{yong2015} (and discussed in Sect. \ref{sec:back}) may include the uncertainty introduced by the actual physical instability in regions with large variability (e.g., the equatorial Pacific)和may unnecessarily flag potential errors in regions with little to no variability due to the denominator in Eq. (\ref{e:rmsz}). 
Note the range of variability shown in Fig. \ref{fig:SST_STD_all} is certainly resolution dependent,和the results in Fig. \ref{fig:SST_STD_all} are specific to the rather dissipative low-resolution ocean model (e.g., 1\degree\space CESM-POP) used in most climate studies.

Therefore to create an ensemble statistical consistency test that is robust for CESM-POP data, we must create a distribution describing the ensemble that contains spatial as well as temporal information. In particular, POP-ECT creates an ensemble file with $T$ monthly time slices of CESM-POP data that contains:  
\begin{itemize}
 \item $N_{var} \times N_X \times T$  monthly mean values across the ensemble at each grid point $i$ ($\mu_i$)
 \item $N_{var} \times N_X \times T$ standard deviations of ensemble monthly mean values at each grid point $i$ ($\sigma_i$),
 \end{itemize}
which is to say that we retain the ensemble mean和standard deviation at each of $N_x$ grid points (note that $N_X$ depends on whether $X$ is a 2D or 3D variable) for the specified number of months, e.g. $T \;=\; 36$.

Finally, we note that without special treatments, the CESM-POP could generate unrealistic salinity distributions in the closed marginal seas because there is no appreciable freshwater feedback between the freshwater和salinity (e.g. Hudson Bay, the Mediterranean Sea).  The current CESM-POP imposes a strong freshwater restoring in the uncoupled simulation和a marginal sea freshwater balancing strategy in the coupled simulation.  These specific treatments can maintain a salinity balance but act as artificial forcings to the model dynamics.  Therefore, in this work we do not address the complexity of how to do verification properly in the marginal seas和instead restrict our attention to the open oceans.

\subsection{Testing procedure}

Given the POP-ECT summary file, we determine whether new simulation output data (e.g. from a code modification, a new machine, a new compiler option) is statistically consistent with the ocean climate categorized by the reference ensemble distribution as follows.  Because we only have five diagnostic variables that are well-understood, we do not need to use PCA. % as in CAM-ECT.  
Instead,  we take the approach of evaluating the standardized difference between the ensemble和the new run \textit{at each grid point}. For each grid point $i$和each new variable $\tilde{{X}}$, we calculate the distance between $\tilde{{X}}$和the ensemble data via a standard Z-score measurement for a given monthly time slice $t$.
In particular, given the values of  $\tilde{{X}}$ at time $t$, $\tilde{{X}} = \{ \tilde{x}_{1,t}, \tilde{x}_{2,t}, \dots, \tilde{x}_{N_X,t}\}$, the Z-score at grid point $i$ for variable $\tilde{{X}}$ at time $t$ is
\begin{equation*}
Z_{\tilde{x}_{i,t}}=  \frac{\tilde{x}_{i,t} -\mu_{i,t}}{\sigma_{i,t}},
\end{equation*}
这里 $\mu_{i,t}$和$\sigma_{i,t}$ are the ensemble mean和standard deviation respectively, at grid point $i$ for variable $X$ at the specified month $t$ as specified in the ensemble summary file.

Now for a particular time slice $t$, we drop all subscripts $t$ from relevant variables, e.g. the Z-score becomes $Z_{\tilde{x_i}}$.
We define an allowable tolerance $tol_{Z}$ for the Z-score at each point, meaning that if $Z_{\tilde{x_i}} > tol_{Z}$, then point $i$ is denoted a ``failed'' point. 
Recall that a Z-score indicates the number of standard deviations away from the mean,和a large Z-score indicates that the new case is far from its climate state in the ensemble.
%By default we choose  $tol_{Z} \; = \; 3.0$, meaning that the new value must be within $3.0$ standard deviations of the ensemble mean at each grid point $i$ to ''pass''.
Next we look at the overall percentage of grid points that have passing  Z-scores, defining the Z-score Passing Rate (ZPR) for variable $\tilde{X}$  as:
\begin{equation}\label{e:zpr}
ZPR_{\tilde{X}} = \frac{ \#\{i \;|\; \tilde{x_i} \in \tilde{X} \; \land \; |Z_{\tilde{x_i}}| \; \leq \; tol_{Z}\} }{\#\{i \;|\; \tilde{x_i} \in \tilde{X} \} }.
\end{equation}
To make an overall determination of whether variable $\tilde{X}$ passed, we set a minimum threshold for the ZPR ($min_{ZPR}$).  In particular, if $ZPR_{\tilde{X}} \geq min_{ZPR}$ then variable $\tilde{X}$ passes.  
By default,  the Z-score tolerance is $tol_{Z} \; = \; 3.0$,和the ZPR threshold is $min_{ZPR} \; = \; 0.9$.  In other words, 90$\%$ of the new values for variable $\tilde{X}$ must be within $3.0$ standard deviations of the ensemble mean ($\mu_i$) at each grid point $i$ for $\tilde{X}$ to ''pass''.  This process is repeated for all five independent diagnostic variables,和all variables must pass for the overall simulation to be deemed statically consistent.

Note that the calculated Z-scores change with simulation length. Because of the longer time-scales present in the ocean, we ran the CESM simulations for most of the experiments in the paper for 36-months. % longer than the 12-months that CAM-ECT requires for CAM data.
 In addition, we output monthly time slice data for POP-ECT (as opposed to the annual temporal mean for CAM-ECT) to determine whether the ensemble ocean states stabilize (or not) over time.  

As will be evident in the following section, the ZPRs generally become stable after a few months,和the stability trends across the diagnostic variables are similar.  Therefore, in addition to picking a suitable Z-score tolerance和passing rate, we choose a checkpoint ($t_C$) at which to evaluate the new run result (instead of checking at all $T$ months of data).  Note that the length of the ensemble simulations does not need to be longer than $t_C$.


\subsection{Software tools}

To make our new POP-specific testing methodology accessible to both users和developers, we added POP-ECT to the existing CESM-ECT suite of Python tools (pyCECT v2.0) , which are included in the CESM public releases. The CESM-ECT Python tools include the tools that create the CESM module-specific ensemble summary files as well as pyCECT, which performs the statistical consistency test using the specified ensemble summary file.  Because the POP-ECT summary file is distinct from the CAM-ECT summary file, we created the parallel Python code pyEnsSumPop to generate the POP-ECT summary files.  In particular, from an ensemble of CESM-POP simulation output files, pyEnsSumPop creates the ensemble summary file (in parallel) containing the ocean model statistics as described in Sect. \ref{sec:ensemble}.  The CESM Software Engineering Group creates a new ensemble of POP simulation data as needed, which currently coincides with the release of a software tag that contains modifications known to alter the climate from the previously tagged version's climate.  The appropriate POP-ECT ensemble summary files are included in development和release tags for CESM as noted in Sect. \ref{sec:code}.  Given a POP-ECT summary file, a user or developer can then evaluate ``new'' simulation data for consistency using the pyCECT Python tool, which is now able to evaluate results based on either the POP-ECT or CAM-ECT methodology.  New CESM-POP simulation data to be evaluated may be the result of using a new architecture or a different compiler option, making a code modification, or changing the input data deck.  pyCECT evaluates whether the new ocean model simulation results are statistically consistent with the specified POP-ECT ensemble和issues an overall ``pass'' or ``fail'' designation.  In addition, the Z-score passing rate is given for each ocean model variable at the selected checkpoint time $t_C$.


% %-----------------------------------------------------------------------------
% % -----------------------------------------------------------------------------
% %----------------------------------------------------------------------------
\section{Experiments} \label{sec:exp}

%As with applying CESM-ECT  to CAM data, the most straightforward approach to verifying the method itself is to apply CESM-ECT to cases with expected outcomes. 

% The primary objective of this section is to evaluate the new POP-ECT tool on CESM-POP simulation data with a series of experiments on configurations with expected outcomes, including revisiting the effect of changing the barotropic solver convergence tolerance.  Experiments were run with the CESM 1.2.2 release, using CESM-POP for the active ocean component, the CICE model for the active sea ice component,和data-driven atmosphere和land components.  In addition, we use the ``normal year'' forced testing framework for ocean-ice simulations, which means that there are no ENSO (El Ni\~{n}o Southern Oscillation) events和variance in the equatorial Pacific may be artificially suppressed.  (Note that this particular CESM component configuration is referred to in CESM documentation as a ``G\_NORMAL\_YEAR'' component set).
% The CESM grid resolution was ``T62\_g16'', which corresponds to a 1\degree grid ($320 \times 384$) for the ocean和ice components, with $60$ vertical levels和a displaced Greenland pole.  Simulations were run on 96 processor cores (unless otherwise specified) on the Yellowstone machine at NCAR.

The primary objective of this section is to evaluate the new POP-ECT tool on CESM-POP simulation data with a series of experiments on configurations with expected outcomes, including revisiting the effect of changing the barotropic solver convergence tolerance. Experiments were run with the CESM 1.2.2 release, using CESM-POP for the active ocean component, the CICE model for the active sea ice component,和data-driven atmosphere和land components. In addition, we use the climatologically-averaged atmospheric forcing (one-year repeating forcing) framework for ocean-ice simulations. Therefore, there are no year-to-year corresponding events (such as El Ni\~{n}o Southern Oscillation),和the variance in the equatorial Pacific may be artificially suppressed. (Note that this particular CESM component configuration is referred to in CESM documentation as a ``G\_NORMAL\_YEA`` component set). The CESM grid resolution was “T62\_g16”, which corresponds to a 1\degree grid ($320 \times 384$) for the ocean和ice components, with $60$ vertical levels和a displaced Greenland pole. Simulations were run on $96$ processor cores (unless otherwise specified) on the Yellowstone machine at NCAR. 


For these experiments, we evaluate 36 months of data as opposed to a single time slice to provide insight as to how the ZPRs vary over time和guide the selection of $t_C$.  Further, to illuminate the relationship between the Z-score和simulation month in terms of ZPR和guide the selection of $tol_{Z}$和$min_{ZPR}$, we utilize a Response Surface Methodology (RSM) \citep[e.g.,][]{box2007}. That is, we provide plots of the response surfaces for variable $\tilde{X}$ 这里 the percentage of grid points $i$ that meet the Z-score tolerance criteria, $Z_{\tilde{x_i}} > tol_{Z}$, are shown with a cumulative distribution function (CDF) for a range of $tol_{Z}$ values和simulation months. Finally, as noted previously, we find an ensemble size of $40$ to be sufficient for our experiments, but we further explore和discuss the ensemble size parameter selection in Sect. \ref{sec:ens}.

For simplicity, we show results for temperature (TEMP)和sea surface height (SSH).  Though we analyzed the other variables as well, these two are representative of the ocean system model in general as SSH is related to ocean circulation dynamics和TEMP is determined by model scalar transport. 
%(Note that results for UVEL和VVEL  look quite similar to temperature.)

\subsection{Barotropic solver convergence tolerance}

First we use the newly enhanced CESM-ECT to revisit the effect of changing the barotropic solver convergence tolerance, as discussed in Sect. \ref{sec:back} in reference to the work in \cite{yong2015}. The default barotropic solver convergence tolerance in CESM-POP is $10^{-13}$,和we ran experiments with convergence tolerances ranging from  $10^{-9}$ to $10^{-16}$, outputting monthly temporal averages at each grid point for 36 months.  
%Changing the convergence tolerance for the barotropic solver corresponds to a perturbation of the forcing term at each time step (c.f. Section \ref{sec:pert}).  
We expect convergence tolerances tighter than the default $10^{-13}$ to result in a consistant climate, but looser tolerances to introduce some error.

Response surfaces for TEMP和SSH are given in Fig. \ref{fig:RSM-TEMP-tol}和Fig. \ref{fig:RSM-SSH-tol}, respectively.  Each figure contains four response surfaces: the original default convergence tolerance ($10^{-13}$)和a tighter tolerance ($10^{-16}$) in the top two subplots和looser tolerances ($10^{-10}$和$5.0*10^{-9}$) in the bottom two subplots.  For each response surface, the x-axis indicates the simulation month (ranging from 1 to 36),和the y-axis indicates the range of Z-score values used for $tol_{Z}$ when calculating the percentage of grid points that fall below the Z-score tolerance, i.e. ZPR in Eq. (\ref{e:zpr}) . The color bar indicates the ZPR as a percentage in increments of 10\%.  The response surface plots are useful for evaluating various combinations of options for $tol_{Z}$和$min_{ZPR}$. For example, consider the effect on variable TEMP of modifying the solver convergence tolerance.  The upper left subplot in Fig. \ref{fig:RSM-TEMP-tol} indicates that for the original convergence tolerance ($10^{-13}$),  90\% of all grid points had a Z-score of less than 2.0 at all simulation months.  In contrast, the subplot below for $10^{-10}$ shows that after the first 9 simulation months, 90\% of the grid points have a Z-score less 3.0,和by 12 months, between 70和80 percent of the grid points have Z-scores less than 2.0. Further loosening the convergence tolerance to $5.0*10^{-9}$ as in the lower right subplot shows pronounced errors in terms of the relatively low ZPR percentages.  If we turn our attention to SSH in Fig. \ref{fig:RSM-SSH-tol} for the same four convergence tolerances, the overall trends are similar.  In particular, for $10^{-13}$, 90\% of all grid points have a Z-score of less than 2.0 at all simulation months (except month 6).  Similarly to TEMP, the subplot for $10^{-10}$ shows that errors have been introduced和errors are even more pronounced for $5.0*10^{-9}$. A notable difference between the response surfaces for TEMP和SSH is that the plots for temperature are smoother over time because diffusion is an important process in the temperature calculation.

If we fix the Z-score tolerance for the data shown in Fig. \ref{fig:RSM-TEMP-tol}和Fig. \ref{fig:RSM-SSH-tol}, we can more easily evaluate the ZPR.
Consider setting $tol_{Z} \; = \; 3.0$, a rather conservative choice. Fig. \ref{fig:PRZ-tol} illustrates the percentage of grid points with Z-scores that exceed $tol_{Z} \; = \; 3.0$ ( i.e. \textit{fail}) for both TEMP和SSH. If we choose a ZPR threshold of $min_{ZPR} \; = \; 0.9$, which corresponds to a 10\% failure rate in Fig. \ref{fig:PRZ-tol}, it is clear that a convergence tolerance of $10^{-10}$ is borderline in terms of passing or failing (and therefore should not be used in practice).  Whereas tolerances tighter than $10^{-10}$ have low failure rates和appear statically consistent with the original tolerance for both variables.  This plot in Fig. \ref{fig:PRZ-tol} is of interest as well as it nicely demonstrates that as the convergence tolerance becomes less strict, the number of grid points exceeding the Z-score tolerance increases.  This result is much clearer than in \cite{yong2015}.

\subsection{Processor layouts}

While CESM simulations that are identical except for differing numbers of CESM-POP processor cores yield non-BFB identical results, the results from such simulations 
should represent the same climate state (i.e., they should not be statistically distinguishable). Here we verify that such simulations definitively pass CESM-ECT.  Recall that the simulations comprising our CESM-ECT ensemble were run on 96 cores.  We ran additional simulations on 48, 192,和384 cores.  Note that we are not using threading in CESM-POP at this time.

The response surface plots for 96 cores (labeled ``original'')和384 cores are the top subplots in Fig. \ref{fig:RSM-TEMP-param}和Fig. \ref{fig:RSM-SSH-param} for TEMP和SSH, respectively. These plots show that for both core counts, 90\% of all grid points have a Z-score of less than 2.0 for nearly all simulation months,和as expected, there is little discernable difference between the two core counts for both variables.  As before, we fix the Z-score tolerance at $tol_{Z} \; = \; 3.0$和show the Z-score failure rates for TEMP with all four core count options (48, 96, 192,和384) in Fig. \ref{fig:combine}.  As anticipated, the failure percentages are quite low (below 1.2\%) for all configurations at all monthly time slices, confirming that differences in simulation output due to varying the core count in CESM-POP are not statistically significant和correctly identified as such by the new CESM-ECT methodology.  Note that the corresponding plot for SSH is not provided as is looks similarly good in terms of very low failure rates.

 \subsection{Physical parameters}\label{sec:pp}

 Now we change two physical parameters expected to alter the ocean climate from the tracer equations: the tracer's vertical mixing coefficient for convective instability和the tracer advection scheme.  Results from these modifications should fail the CESM-ECT. First, by default, the vertical mixing coefficient for convective instability (\textit{convect\_diff}) is set to be \textit{convect\_diff} $=\; 10,000$ for the tracer mixing coefficient in the 1\degree CESM-POP configuration.  We increase this parameter by factors of 2, 5,和10, which is expected to increase the vertical mixing in the ocean interior when the density profile is unstable. This should noticeably impact the CEM-POP results due to the different mixing property.  Second, we change the POP tracer advection scheme (\textit{t\_advect\_ctype}) from the default 3rd-order upwind scheme (\textit{upwind3}) to the Lax-Wendroff scheme with 1D flux limiters (\textit{lw\_lim}). This change is also significant和should lead to a different climate state because the associated diffusion和dispersion errors differ.

The response surface plots for increasing \textit{convect\_diff} by a factor of 10 are given in the lower left subplots in Fig. \ref{fig:RSM-TEMP-param}和Fig. \ref{fig:RSM-SSH-param} for TEMP和SSH, respectively.  This change clearly affects the climate state significantly, particularly as compared to changing the CESM-POP core count to 384 as depicted in the upper right subplot in both figures.  In fact, the impact on TEMP of increasing \textit{convect\_diff} in Fig. \ref{fig:RSM-TEMP-param} is almost as strong as changing the solver convergence tolerance to $10^{-9}$ in Fig. \ref{fig:RSM-TEMP-tol}.  The change of the advection scheme also leads to different climate state, evident in the lower right subplots in Fig. \ref{fig:RSM-TEMP-param}和Fig. \ref{fig:RSM-SSH-param} for TEMP和SSH, respectively.  Note that the Z-scores at nearly every grid point are failing.

The Z-score failure rates for $tol_{Z} \; = \; 3.0$ are shown in Fig. \ref{fig:PRZ-temp-param} for advection scheme change as well as all the modifications to the tracer vertical mixing coefficient for convective instability.  If we choose a ZPR threshold of $min_{ZPR} \; = \; 0.9$, which corresponds to a maximum of 10\% failure rate,  then doubling the vertical mixing coefficient (\textit{convect\_diff}*2) is borderline in terms of passing or failing.  The remaining tests clearly fail for both TEMP和SSH, as expected. Based on our experiments thus far, choosing a Z-score tolerance of $tol_{Z} \; = \; 3.0$和a ZPR threshold of $min_{ZPR} \; = \; 0.9$ yields the expected outcome,和these parameter settings are the default for the pyCECT tool.  


 \subsection{Simulation length}


%In particular, in cases which should pass (such as changing the core counts), more than 90\% of the points have Z-scores smaller than 3.0 at all monthly time slices.  While for cases which use different physical schemes (Section \ref{sec:pp}), the percentage of points which have Z-scores smaller than 3.0 is significantly lower than 90 \%.

 Our experiments in Fig. \ref{fig:PRZ-temp-param} indicate that the percentage of grid points with failing Z-scores differs little from month to month after the first 12 months for both TEMP和SSH.  This conclusion can also be reached from Fig. \ref{fig:RSM-TEMP-param}和Fig. \ref{fig:RSM-SSH-param} for TEMP和SSH, respectively.  In particular, the response of SSH to the initial temperature perturbation is largely stabilized after 12 months. The SSH may be affected through the circulation change resulting from the change of density stratification.  Based on our experimental results, evaluating the output at a single well-chosen checkpoint time $t_C$ appears reasonable.

 We generally choose $t_C \;=\; 12$ to minimize the computational requirements of creating the ensemble for each candidate CESM tag. (The ensemble simulations runs can be length $t_C$.)  Consider the surface plots for month $t_C \;=\; 12$ in Fig. \ref{fig:zscore-combine} that illustrate the Z-score values for SST as compared to the ensemble for four different model configurations.  The top subplot is the original case.  The second plot from the top is the result of changing the number of CESM-POP processor cores to 384, which resembles the topmost plot as expected.  While the patterns are not identical for the upper two plots, the Z-score magnitudes和distributions are similar, indicating a degree of statistical consistency when changing the number of processors.  In contrast, increasing the tracer mixing coefficient for convective instability by a factor of 10 was shown to change the climate state in Sect. \ref{sec:pp},和this result is clearly evident in the Z-score at month 12 in the third plot from the top of Fig. \ref{fig:zscore-combine}.  Finally, the bottom subplot in Fig. \ref{fig:zscore-combine} indicates a largely altered climate state due to the use of a different advection scheme, which corroborates the substantial effects seen in Fig. \ref{fig:RSM-TEMP-param}和Fig. \ref{fig:RSM-SSH-param}.  Using a different advection scheme significantly changes the numerical dissipation和diffusion associated with the scheme \citep{tseng2008}和effectively influences the circulation pattern和structure in the ocean model \citep[e.g.,][]{tseng2006}. 
In particular, the Lax-Wendroff scheme with flux limiters can introduce excessive numerical mixing which may interact with the physical mixing of temperature和salinity, though it can result in a much smoother solution in general.


% %-----------------------------------------------------------------------------
% % -----------------------------------------------------------------------------
% %----------------------------------------------------------------------------
\section{Ensemble size } \label{sec:ens}

The size (i.e., number of members) of the ensemble must be large enough to sufficiently capture ocean model variability, but as small as possible for computational efficiency.  In this section, we discuss the sensitivity of POP-ECT to ensemble size.  We setup experiments to determine the false positive rate associated with multiple ensembles sizes as follows.
First, we generate a total of 80 ensemble members that differ by an $\mathcal{O}(10^{-14})$ perturbation to the initial ocean temperature field.  Second, from the 80 members, we remove 10 to use as our test set. Next, from the remaining 70 members, we create ensembles of sizes 10, 20, 30, 40, 50,和60.  In particular, for each ensemble size, we do 100 random draws for each ensemble size from the set of 70 members, resulting in 100 distinct ensembles corresponding to each ensemble size. Then for each ensemble size, we run POP-ECT at $t_c = 12$ months for each of the 10 members of the test set with all 100 ensembles of that size, resulting in 1000 tests per ensemble size.   We consider the measured experimental failure rate to be the type I error, or ``false positive'' rate.  Because the test set和the ensemble members are all drawn from the larger 80 member collection that represents a statistically consistent climate, the Z-score failure rate would ideally be as low as possible. 

Figure \ref{fig:temp_ens_80} shows the results of performing these experiments for variables TEMP和SSH. The x-axis indicates the ensemble size,和the y-axis indicates the Z-score failure rate.  For each ensemble size, the squares denote the mean和the error bars indicate one standard deviation of uncertainty.  As expected,  as the ensemble size increases, the false positive rate decreases和the range of uncertainty shrinks.  However,  increasing the ensemble size has diminishing returns; the improvement in false positive rate when using 20 instead of 10 members is much greater than the improvement gained by using 60 instead of 50 members.  We choose an ensemble size of 40 as improvement beyond that is marginal和we balance a low false positive rate with keeping the cost of ensemble generation low.

% %-----------------------------------------------------------------------------
% % -----------------------------------------------------------------------------
% %----------------------------------------------------------------------------
 \conclusions[Conclusions和Future Work]\label{sec:concl}


Because the CESM-POP ocean model is widely-used和critical to many climate simulations, assuring its quality is critical. However, the chaotic nature of ocean dynamics often leads to simulation results that are not identical in the presence of minor differences, such as a change in processor core count for the simulation.  Therefore, the ability to easily determine whether differences in model results are statistically significant is important to both climate scientists和model developers.  The ensemble methodology developed for evaluating consistency with atmospheric data, CAM-ECT, was not appropriate for ocean simulation data based on its differing characteristics.  Therefore, we developed a new ocean model-specific methodology for statistical consistency testing, POP-ECT, that allows for the subjective detection of statistically significant changes in CESM-POP.  Together with the new methodology, using an appropriately-sized ensemble is critical as well. Our experiments indicate the appropriateness of the new approach for detecting differences in the model ocean state. The addition of POP-ECT to the CESM-ECT suite of tools has greatly enhanced the capability to ensure quality CESM simulations.

We plan to extend this work in a number of ways.  First, the existing spatial approach lends itself to the examination of regional ocean diagnostics.  For example, named oceans could be identified individually as the source of failure if the global test fails.  Enabling the move from coarse- to fine-grain diagnostics would facilitate determining the root cause of an error or difference.  Second, we plan to extend the evaluation of the effects of data compression on climate data in \cite{baker2014} to ocean model data和will use the testing methodology presented here to evaluate the impact. The ability to determine whether changes in the ocean state are statically significant or not due to data loss during compression is critical to the acceptance of compression as a tool to reduce data volumes for ocean simulation data.


% % -----------------------------------------------------------------------------
% % ----------------- Section Code--------------------------------------
% % -----------------------------------------------------------------------------

  \section{Code availability}\label{sec:code}

The CESM-ECT Python tools (pyCECT v2.0) can be obtained independently of CESM from NCAR's public git repository (\url{https://github.com/NCAR/PyCECT/releases}).  The version of CESM used for our experiments, CESM 1.2.2, is available at \url{http://www.cesm.ucar.edu/models/cesm1.2}. The CESM-ECT software tools are also included in the CESM public releases, with the POP-ECT addition available starting with the CESM 2.0 release series. 
%Alternatively, the tool can also be obtained as a subset of the CIME tools (statistical\_ensemble\_test), which are available at {\url{https://github.com/CESM-Development/cime}.  
CESM-POP simulation data is available from the corresponding author upon request.


 

\bibliographystyle{copernicus}
 \bibliography{pv}

% %------------------------------------------------------------------------------

%Figures和tables at the end

\clearpage
\begin{figure}[h]
\includegraphics[height=6.5cm]{fig/temp_rmse_GMD.png}
\caption {Monthly Root Mean Square Error (RMSE) of temperature for experiments with different barotropic solver convergence tolerances. Note that this is a subset of Fig. 12 in \cite{yong2015}.}
\label{fig:rmse_temp}
\end{figure} 

\clearpage
 \begin{figure}[h]
 \begin{center}
 \includegraphics[height=6.5cm]{fig/temp_rmsz_GMD.png}
 \end{center}
 \caption {Monthly Root Mean Square Z-score (RMSZ) of temperature with respect to an ensemble (denoted by yellow) for experiments with barotropic different convergence tolerances. Note that this is subset of Fig. 13 in \cite{yong2015}. }
 \label{fig:rmsz_temp_ens}
 \end{figure} 

\clearpage
\begin {figure}[h]
\centering
\includegraphics[height =6.5cm]{fig/pert-force.png}
\caption{The top panel displays four analytic solutions to Eq. (\ref{e:2}) with the indicated perturbations to the initial conditions (P)和perturbation to the forcing term (F).  Note that all four perturbations have similar effect on the ``original'' (unperturbed) solution.  The bottom panel plots the error between the four perturbations和the original solution. In this example, the error due to perturbing the forcing term (F) is constant和smaller in magnitude that the errors caused by the initial condition perturbations.}
\label {fig:1Danalytical}
\end {figure}

\clearpage
\begin {figure}[h]
\centering
\includegraphics[height =20cm]{fig/gimp_all_sst_std.png}
\caption{The ensemble distribution for the standard deviation of sea surface temperature (SST) at months 1, 12, 24,和36.}
\label{fig:SST_STD_all}
\end {figure}

\clearpage
\begin {figure}[h]
\centering
\includegraphics[height =12cm]{fig/RSM-TEMP-tol.png}
\caption {Response surfaces for Z-score of temperature (TEMP) over time (monthly)和 Z-score tolerance.  Each subplot represents a different barotropic
solver convergence tolerance (labeled above). The color bar indicates the percentage of grid points with a Z-score below the Z-score tolerance (on the y-axis).}
\label{fig:RSM-TEMP-tol}
\end {figure}

\clearpage
\begin {figure}[h]
\centering
\includegraphics[height =12cm]{fig/RSM-SSH-tol.png}
\caption {Response surface for Z-score of sea surface height (SSH) over time (monthly)和Z-score tolerance.  Each subplot represents a different barotropic
solver convergence tolerance (labeled above). The color bar indicates the percentage of grid points with a Z-score below the Z-score tolerance (on the y-axis).}
\label{fig:RSM-SSH-tol}
\end {figure}


\clearpage
\begin{figure}[h]
\centering 
\includegraphics[height=7cm]{fig/prz_tol_combined.png}
\caption {Percentage of grid points with Z-scores for temperature
  (TEMP)和sea surface height (SSH) that exceed the 3.0 tolerance for simulations with various barotropic solver convergence tolerances. }
\label {fig:PRZ-tol}
\end{figure}

\clearpage
\begin{figure}[h]
\includegraphics[height=7.0cm]{fig/temp_cores_zoom_combine.png}
\caption{Percentage of grid points with Z-scores for temperature (TEMP) that exceed the 3.0 tolerance for simulations with various numbers of processor cores. (Note that the left和right subplots contain the same information, with different scales for the y-axis.)}
\label {fig:combine}
\end{figure}

\clearpage
\begin {figure}[h]
\centering
\includegraphics[height =12cm]{fig/RSM-TEMP-param.png}
\caption {Response surfaces for Z-score of temperature (TEMP) over time (monthly)和 Z-score threshold.  The top two subplots represent two different processor core layouts.
The bottom left has a tracer mixing coefficient for convective instability that is 10 times larger than the original,和the bottom right uses a different tracer advection scheme. The color bar indicates the percentage of grid points with a Z-score below the Z-score tolerance (on the y-axis).}
\label{fig:RSM-TEMP-param}
\end {figure}

\clearpage
\begin {figure}[h]
\centering
\includegraphics[height =12cm]{fig/RSM-SSH-param.png}
\caption {Response surfaces for Z-score of sea surface height (SSH) over time (monthly)和 Z-score threshold.  The top two subplots represent two different processor core layouts.
The bottom left has a tracer mixing coefficient for convective instability 10 times larger than the original,和the bottom right uses a different tracer advection scheme. The color bar indicates the percentage of grid points with a Z-score below the Z-score tolerance (on the y-axis).}
\label{fig:RSM-SSH-param}
\end {figure}


\clearpage
\begin{figure}[h]
\centering
\includegraphics[height=7cm]{fig/prz_param_combined.png}
\caption{Percentage of grid points with Z-scores for temperature
  (TEMP)和sea surface height (SSH) that exceed the 3.0 threshold for simulations with an alternative tracer advection scheme (lw\_lim)和several tracer mixing coefficients for convective instability.}
\label {fig:PRZ-temp-param}
\end{figure}


\clearpage
\begin {figure}[h]
\centering
\includegraphics[height =20cm]{fig/zscore_sst_combine_crop.png}
\caption{Z-score of sea surface temperature (SST) at month 12 for the
original (default) case, a 384 processor core case, a case with a
  larger tracer mixing coefficient for convective instability,和a case with an alternate tracer advection scheme.}
\label {fig:zscore-combine}
\end {figure}

\clearpage
\begin {figure}[h]
\centering
\includegraphics[height =6.5cm]{fig/ens_size_combined.png}
\caption{The distributions of experimental failure rates based on 1000 tests for variables temperature (TEMP)和sea surface height (SSH).  For each ensemble size, the green bars indicate the maximum和minimum values obtained,和the red boxes indicate the mean.   }
 \label {fig:temp_ens_80}
\end {figure}



% %------------------------------------------------------------------------------

 \end{document}

